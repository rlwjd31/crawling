{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579ed319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, TypedDict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ignore InsecureRequestWarning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "ROOT = \"https://www.riss.kr/\"\n",
    "PATH = \"search/Search.do\"\n",
    "param_dict = {\n",
    "  \"isDetailSearch\": 'N',\n",
    "  \"searchGubun\": True,\n",
    "  \"viewYn\": 'OP',\n",
    "  \"strQuery\": '엣지 디바이스',\n",
    "  \"order\": '/DESC',\n",
    "  \"onHanja\": False,\n",
    "  \"strSort\": 'RANK',\n",
    "  \"iStartCount\": 0,\n",
    "  \"fsearchMethod\": 'search',\n",
    "  \"sflag\": 1,\n",
    "  \"isFDetailSearch\":'N',\n",
    "  \"pageNumber\": 1,\n",
    "  \"icate\": 're_a_kor',\n",
    "  \"colName\": 're_a_kor',\n",
    "  \"pageScale\": 10,\n",
    "  \"isTab\": 'Y',\n",
    "  \"query\": '엣지 디바이스',\n",
    "}\n",
    "\n",
    "\n",
    "HeaderType = TypedDict(\"HeaderType\", {\"User-Agent\": str, \"Referer\": str})\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PageResponseReturnType:\n",
    "  response: requests.Response\n",
    "  soup: BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page_response_with_soup(url: str, *, query_params: Optional[dict[Any, Any]] = None, header: Optional[HeaderType] = None) -> PageResponseReturnType:\n",
    "  # 👇 verify=False로 통해 SSL 검증 무시, playwright나 selenium은 브라우저를 띄워서 사용하기 때문에 verify=False일 시 해당 library를 사용하여 우회 가능.\n",
    "  response = requests.get(url, params=query_params, headers=header, verify=False)\n",
    "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "  return PageResponseReturnType(response=response, soup=soup)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51416765",
   "metadata": {},
   "source": [
    "# pagination 하면서 상세 논문 페이지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a48865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8466eab83408d9c7f9f68d1fd215a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "잰행률:   0%|          | 0/40 [00:00<?, ?paper/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "result = []\n",
    "PAGE_NUM = int(input()) \n",
    "PAPER_COUNT_PER_PAGE = 10\n",
    "\n",
    "# progress bar 표시\n",
    "crawling_progress_bar = tqdm(total=PAGE_NUM * PAPER_COUNT_PER_PAGE, desc=\"잰행률\", unit=\"paper\", dynamic_ncols=True, miniters=1)\n",
    "\n",
    "for page_count in range(1, PAGE_NUM + 1):\n",
    "  paper_list_response = get_page_response_with_soup(urljoin(ROOT, PATH), query_params={\n",
    "    **param_dict,\n",
    "    \"pageNumber\": page_count\n",
    "  })\n",
    "\n",
    "  previous_link, paper_list_soup = paper_list_response.response.url, paper_list_response.soup\n",
    "  paper_list_element = paper_list_soup.select(\".srchResultListW > ul >  li\")\n",
    "\n",
    "  for paper_element in paper_list_element:\n",
    "    title = paper_element.select_one(\".title > a\").get_text(strip=True)\n",
    "    link = urljoin(ROOT, paper_element.select_one(\".title > a\").get(\"href\"))\n",
    "    # print(f\"================================ visiting detail paper {title} =================================\")\n",
    "    paper_detail_response = get_page_response_with_soup(link, header={\n",
    "      \"User-Agent\": \"Mozilla/5.0\",\n",
    "      \"Referer\": previous_link\n",
    "    })\n",
    "    paper = paper_detail_response.soup\n",
    "    press = paper.find(\"span\", string=\"발행기관\").find_next_sibling().get_text(strip=True)\n",
    "    year = paper.find(\"span\", string=\"발행연도\").find_next_sibling().get_text(strip=True)\n",
    "    # 👇 detail paper page에 따라 주제어가 있는 곳도 없는 곳도 존재하여 분기처리\n",
    "    keywords = [] if not paper.find(\"span\", string=\"주제어\") else [keyword.strip() for keyword in paper.find(\"span\", string=\"주제어\").find_next_sibling().text.split(\";\")]\n",
    "\n",
    "    result.append([title, link, press, year, keywords])\n",
    "    crawling_progress_bar.update(1) # 👉🏻 논문 한 건 처리시마다 1씩 증가\n",
    "    # print(title, link, press, year, keywords, sep=\"\\n\")\n",
    "    # print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e1932",
   "metadata": {},
   "source": [
    "# Save to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e944c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"제목\", \"링크\", \"발행기관\", \"발행연도\", \"키워드\"])\n",
    "df.to_excel(\"./outputs/riss_paper.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b38044",
   "metadata": {},
   "source": [
    "# dataclass vs TypedDict\n",
    "\n",
    "- TypedDict: dictionary형태의 문법과 추론을 지원하는 타입 흰트 문법\n",
    "  - 함수형 문법\n",
    "    - 이중 dictionary의 타입을 클래스형은 추론 못 하지만 함수형 문법은 추론 가능함.\n",
    "    ```python\n",
    "    HeaderType = TypedDict(\"HeaderType\", {\"User-Agent\": str, \"Referer\": str})\n",
    "    ```\n",
    "  - 클래스형 문법 :\n",
    "    ```python\n",
    "    class HeaderType(TypedDict):\n",
    "      User-Agent: str\n",
    "      Referer: str\n",
    "    ```\n",
    "\n",
    "- dataclass\n",
    "  - 함수형 문법: @dataclass를 이용\n",
    "  - TypedDict과 다르게 HeaderType(response = ... , soup = ...) 처럼 명시적으로 타입을 지정하여 사용할 수 있음.\n",
    "  ```python\n",
    "    @dataclass(frozen=True)\n",
    "    class PageResponseReturnType:\n",
    "      response: requests.Response\n",
    "      soup: BeautifulSoup\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42ee6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
