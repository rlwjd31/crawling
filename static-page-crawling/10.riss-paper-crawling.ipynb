{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579ed319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Optional, TypedDict\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ignore InsecureRequestWarning\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "ROOT = \"https://www.riss.kr/\"\n",
    "PATH = \"search/Search.do\"\n",
    "param_dict = {\n",
    "  \"isDetailSearch\": 'N',\n",
    "  \"searchGubun\": True,\n",
    "  \"viewYn\": 'OP',\n",
    "  \"strQuery\": 'ì—£ì§€ ë””ë°”ì´ìŠ¤',\n",
    "  \"order\": '/DESC',\n",
    "  \"onHanja\": False,\n",
    "  \"strSort\": 'RANK',\n",
    "  \"iStartCount\": 0,\n",
    "  \"fsearchMethod\": 'search',\n",
    "  \"sflag\": 1,\n",
    "  \"isFDetailSearch\":'N',\n",
    "  \"pageNumber\": 1,\n",
    "  \"icate\": 're_a_kor',\n",
    "  \"colName\": 're_a_kor',\n",
    "  \"pageScale\": 10,\n",
    "  \"isTab\": 'Y',\n",
    "  \"query\": 'ì—£ì§€ ë””ë°”ì´ìŠ¤',\n",
    "}\n",
    "\n",
    "\n",
    "HeaderType = TypedDict(\"HeaderType\", {\"User-Agent\": str, \"Referer\": str})\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PageResponseReturnType:\n",
    "  response: requests.Response\n",
    "  soup: BeautifulSoup\n",
    "\n",
    "\n",
    "def get_page_response_with_soup(url: str, *, query_params: Optional[dict[Any, Any]] = None, header: Optional[HeaderType] = None) -> PageResponseReturnType:\n",
    "  # ğŸ‘‡ verify=Falseë¡œ í†µí•´ SSL ê²€ì¦ ë¬´ì‹œ, playwrightë‚˜ seleniumì€ ë¸Œë¼ìš°ì €ë¥¼ ë„ì›Œì„œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— verify=Falseì¼ ì‹œ í•´ë‹¹ libraryë¥¼ ì‚¬ìš©í•˜ì—¬ ìš°íšŒ ê°€ëŠ¥.\n",
    "  response = requests.get(url, params=query_params, headers=header, verify=False)\n",
    "  soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "  return PageResponseReturnType(response=response, soup=soup)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51416765",
   "metadata": {},
   "source": [
    "# pagination í•˜ë©´ì„œ ìƒì„¸ ë…¼ë¬¸ í˜ì´ì§€ í¬ë¡¤ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a48865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d8466eab83408d9c7f9f68d1fd215a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ì°í–‰ë¥ :   0%|          | 0/40 [00:00<?, ?paper/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "result = []\n",
    "PAGE_NUM = int(input()) \n",
    "PAPER_COUNT_PER_PAGE = 10\n",
    "\n",
    "# progress bar í‘œì‹œ\n",
    "crawling_progress_bar = tqdm(total=PAGE_NUM * PAPER_COUNT_PER_PAGE, desc=\"ì°í–‰ë¥ \", unit=\"paper\", dynamic_ncols=True, miniters=1)\n",
    "\n",
    "for page_count in range(1, PAGE_NUM + 1):\n",
    "  paper_list_response = get_page_response_with_soup(urljoin(ROOT, PATH), query_params={\n",
    "    **param_dict,\n",
    "    \"pageNumber\": page_count\n",
    "  })\n",
    "\n",
    "  previous_link, paper_list_soup = paper_list_response.response.url, paper_list_response.soup\n",
    "  paper_list_element = paper_list_soup.select(\".srchResultListW > ul >  li\")\n",
    "\n",
    "  for paper_element in paper_list_element:\n",
    "    title = paper_element.select_one(\".title > a\").get_text(strip=True)\n",
    "    link = urljoin(ROOT, paper_element.select_one(\".title > a\").get(\"href\"))\n",
    "    # print(f\"================================ visiting detail paper {title} =================================\")\n",
    "    paper_detail_response = get_page_response_with_soup(link, header={\n",
    "      \"User-Agent\": \"Mozilla/5.0\",\n",
    "      \"Referer\": previous_link\n",
    "    })\n",
    "    paper = paper_detail_response.soup\n",
    "    press = paper.find(\"span\", string=\"ë°œí–‰ê¸°ê´€\").find_next_sibling().get_text(strip=True)\n",
    "    year = paper.find(\"span\", string=\"ë°œí–‰ì—°ë„\").find_next_sibling().get_text(strip=True)\n",
    "    # ğŸ‘‡ detail paper pageì— ë”°ë¼ ì£¼ì œì–´ê°€ ìˆëŠ” ê³³ë„ ì—†ëŠ” ê³³ë„ ì¡´ì¬í•˜ì—¬ ë¶„ê¸°ì²˜ë¦¬\n",
    "    keywords = [] if not paper.find(\"span\", string=\"ì£¼ì œì–´\") else [keyword.strip() for keyword in paper.find(\"span\", string=\"ì£¼ì œì–´\").find_next_sibling().text.split(\";\")]\n",
    "\n",
    "    result.append([title, link, press, year, keywords])\n",
    "    crawling_progress_bar.update(1) # ğŸ‘‰ğŸ» ë…¼ë¬¸ í•œ ê±´ ì²˜ë¦¬ì‹œë§ˆë‹¤ 1ì”© ì¦ê°€\n",
    "    # print(title, link, press, year, keywords, sep=\"\\n\")\n",
    "    # print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141e1932",
   "metadata": {},
   "source": [
    "# Save to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e944c8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(result, columns=[\"ì œëª©\", \"ë§í¬\", \"ë°œí–‰ê¸°ê´€\", \"ë°œí–‰ì—°ë„\", \"í‚¤ì›Œë“œ\"])\n",
    "df.to_excel(\"./outputs/riss_paper.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b38044",
   "metadata": {},
   "source": [
    "# dataclass vs TypedDict\n",
    "\n",
    "- TypedDict: dictionaryí˜•íƒœì˜ ë¬¸ë²•ê³¼ ì¶”ë¡ ì„ ì§€ì›í•˜ëŠ” íƒ€ì… í°íŠ¸ ë¬¸ë²•\n",
    "  - í•¨ìˆ˜í˜• ë¬¸ë²•\n",
    "    - ì´ì¤‘ dictionaryì˜ íƒ€ì…ì„ í´ë˜ìŠ¤í˜•ì€ ì¶”ë¡  ëª» í•˜ì§€ë§Œ í•¨ìˆ˜í˜• ë¬¸ë²•ì€ ì¶”ë¡  ê°€ëŠ¥í•¨.\n",
    "    ```python\n",
    "    HeaderType = TypedDict(\"HeaderType\", {\"User-Agent\": str, \"Referer\": str})\n",
    "    ```\n",
    "  - í´ë˜ìŠ¤í˜• ë¬¸ë²• :\n",
    "    ```python\n",
    "    class HeaderType(TypedDict):\n",
    "      User-Agent: str\n",
    "      Referer: str\n",
    "    ```\n",
    "\n",
    "- dataclass\n",
    "  - í•¨ìˆ˜í˜• ë¬¸ë²•: @dataclassë¥¼ ì´ìš©\n",
    "  - TypedDictê³¼ ë‹¤ë¥´ê²Œ HeaderType(response = ... , soup = ...) ì²˜ëŸ¼ ëª…ì‹œì ìœ¼ë¡œ íƒ€ì…ì„ ì§€ì •í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŒ.\n",
    "  ```python\n",
    "    @dataclass(frozen=True)\n",
    "    class PageResponseReturnType:\n",
    "      response: requests.Response\n",
    "      soup: BeautifulSoup\n",
    "  ```\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e42ee6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
